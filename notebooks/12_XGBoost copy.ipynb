{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052ea73e",
   "metadata": {},
   "source": [
    "# Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1508fe",
   "metadata": {},
   "source": [
    "## Score: 0.XXXX\n",
    "The target variable is: \n",
    "\n",
    "amount_new_house_transactions: The total monetary value of new house transactions in 10,000 yuan.\n",
    "\n",
    "We are using some of the features found in the different csv's to predict the final amount\n",
    "The method used is XGBoost with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617ba29",
   "metadata": {},
   "source": [
    "# 1. SETUP - Dependencies, dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c024c0",
   "metadata": {},
   "source": [
    "## 1.1 Dependencies and plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ef62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from xgboost.callback import EarlyStopping\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global plot styles\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=12)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac19c3a",
   "metadata": {},
   "source": [
    "## 1.2 Loads all necessary data without any processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed74d60",
   "metadata": {},
   "source": [
    "We first define the datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data path\n",
    "DATA_PATH = Path(\"/Users/nikola/Python/KaggleCompetition/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f759d",
   "metadata": {},
   "source": [
    "We load the complemetary datasets which will include many of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502563ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Data (Monthly)\n",
    "new_house = pd.read_csv(DATA_PATH / \"train\" / \"new_house_transactions.csv\")\n",
    "pre_owned = pd.read_csv(DATA_PATH / \"train\" / \"pre_owned_house_transactions.csv\")\n",
    "land = pd.read_csv(DATA_PATH / \"train\" / \"land_transactions.csv\")\n",
    "\n",
    "# Nearby Sectors Transaction Data\n",
    "new_house_nearby = pd.read_csv(DATA_PATH / \"train\" / \"new_house_transactions_nearby_sectors.csv\")\n",
    "pre_owned_nearby = pd.read_csv(DATA_PATH / \"train\" / \"pre_owned_house_transactions_nearby_sectors.csv\")\n",
    "land_nearby = pd.read_csv(DATA_PATH / \"train\" / \"land_transactions_nearby_sectors.csv\")\n",
    "\n",
    "# Sector Features (Static)\n",
    "sector_poi = pd.read_csv(DATA_PATH / \"train\" / \"sector_POI.csv\")\n",
    "\n",
    "# Market Indicators (Time-series)\n",
    "search_index = pd.read_csv(DATA_PATH / \"train\" / \"city_search_index.csv\")\n",
    "city_indexes = pd.read_csv(DATA_PATH / \"train\" / \"city_indexes.csv\")\n",
    "\n",
    "# Convert month columns to datetime for datasets that have it\n",
    "datasets_with_month = [new_house, pre_owned, land, new_house_nearby, pre_owned_nearby, land_nearby, search_index]\n",
    "\n",
    "for df in datasets_with_month:\n",
    "    if 'month' in df.columns:\n",
    "        df['month'] = pd.to_datetime(df['month'])\n",
    "\n",
    "# Store in dictionary for easy access\n",
    "datasets = {\n",
    "    'new_house': new_house,\n",
    "    'pre_owned': pre_owned,\n",
    "    'land': land,\n",
    "    'new_house_nearby': new_house_nearby,\n",
    "    'pre_owned_nearby': pre_owned_nearby,\n",
    "    'land_nearby': land_nearby,\n",
    "    'sector_poi': sector_poi,\n",
    "    'search_index': search_index,\n",
    "    'city_indexes': city_indexes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a98203",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_house.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2cadaa",
   "metadata": {},
   "source": [
    "# 3. DATA & FEATURE ENGINEERING - Merging, cleaning, handling missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 3.1 Create Master Dataset\n",
    "# ------------------------------------------\n",
    "\n",
    "# Start with new_house as base (contains target)\n",
    "df = new_house.copy()\n",
    "\n",
    "# Define target variable\n",
    "target_col = 'amount_new_house_transactions'\n",
    "\n",
    "# MINIMAL feature selection - only the most essential\n",
    "columns_to_merge = {\n",
    "    'pre_owned': [\n",
    "        'month', 'sector',\n",
    "        'amount_pre_owned_house_transactions',  # Direct substitute market\n",
    "        'price_pre_owned_house_transactions'    # Price signals\n",
    "    ],\n",
    "    'new_house_nearby': [\n",
    "        'month', 'sector',\n",
    "        'amount_new_house_transactions_nearby_sectors'  # Spatial spillover\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Merge only essential datasets\n",
    "for data, name in [(pre_owned, 'pre_owned'), \n",
    "                   (new_house_nearby, 'new_house_nearby')]:\n",
    "    \n",
    "    cols_to_use = [col for col in columns_to_merge[name] if col in data.columns]\n",
    "    df = df.merge(data[cols_to_use], on=['month', 'sector'], how='left')\n",
    "    print(f\"Merged {name}: added {len(cols_to_use)-2} features, shape: {df.shape}\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Base features (excluding target): {df.shape[1] - 1}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3.2 Add Seasonal Dummy Variables\n",
    "# ------------------------------------------\n",
    "\n",
    "# Create dummy variables for peak and low months\n",
    "df['is_december'] = (df['month'].dt.month == 12).astype(int)\n",
    "df['is_february'] = (df['month'].dt.month == 2).astype(int)\n",
    "\n",
    "# Verify the seasonal patterns\n",
    "dec_avg = df[df['is_december'] == 1][target_col].mean()\n",
    "feb_avg = df[df['is_february'] == 1][target_col].mean()\n",
    "other_avg = df[(df['is_december'] == 0) & (df['is_february'] == 0)][target_col].mean()\n",
    "\n",
    "print(f\"Seasonal patterns in {target_col}:\")\n",
    "print(f\"  December (peak): {dec_avg:,.0f}\")\n",
    "print(f\"  February (low):  {feb_avg:,.0f}\")\n",
    "print(f\"  Other months:    {other_avg:,.0f}\")\n",
    "print(f\"\\nAdded 2 dummy variables: is_december, is_february\")\n",
    "print(f\"Updated dataset shape: {df.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3.3 MINIMAL FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "\n",
    "# Sort for lag calculations\n",
    "df = df.sort_values(['sector', 'month']).reset_index(drop=True)\n",
    "\n",
    "# 3. Essential time features\n",
    "df['month_num'] = df['month'].dt.month\n",
    "df['quarter'] = df['month'].dt.quarter\n",
    "df['year'] = df['month'].dt.year\n",
    "\n",
    "# 4. One key ratio\n",
    "if 'price_pre_owned_house_transactions' in df.columns:\n",
    "    df['new_to_preowned_price_ratio'] = (\n",
    "        df['price_new_house_transactions'] / \n",
    "        df['price_pre_owned_house_transactions'].replace(0, np.nan)\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal feature count: {df.shape[1] - 3}\")  # -3 for month, sector, target\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3.6 Create Lagged Features (No Data Leakage)\n",
    "# ------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING LAGGED FEATURES FOR TIME SERIES FORECASTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by sector and time to ensure proper lagging\n",
    "df = df.sort_values(['sector', 'month']).reset_index(drop=True)\n",
    "\n",
    "# Keep original features (we'll need them for generating features on future data)\n",
    "features_to_lag = [\n",
    "    'num_new_house_available_for_sale',\n",
    "    'amount_pre_owned_house_transactions',\n",
    "    'period_new_house_sell_through'\n",
    "]\n",
    "\n",
    "# Create 1-month lags\n",
    "for feature in features_to_lag:\n",
    "    if feature in df.columns:\n",
    "        lag_col_name = f'{feature}_lag1'\n",
    "        df[lag_col_name] = df.groupby('sector')[feature].shift(1)\n",
    "        non_null = df[lag_col_name].notna().sum()\n",
    "        print(f\"{lag_col_name}: {non_null}/{len(df)} non-null values\")\n",
    "\n",
    "# Create sector-level historical averages (no leakage)\n",
    "print(f\"\\nCreating sector historical features...\")\n",
    "df['sector_historical_mean'] = df.groupby('sector')[target_col].transform(\n",
    "    lambda x: x.expanding().mean().shift(1)\n",
    ")\n",
    "df['sector_historical_std'] = df.groupby('sector')[target_col].transform(\n",
    "    lambda x: x.expanding().std().shift(1)\n",
    ")\n",
    "\n",
    "# Create month-level historical averages across all sectors (seasonal patterns)\n",
    "df['month_historical_mean'] = df.groupby(df['month'].dt.month)[target_col].transform(\n",
    "    lambda x: x.expanding().mean().shift(1)\n",
    ")\n",
    "\n",
    "# Sector-month combination average (sector-specific seasonal pattern)\n",
    "df['sector_month_avg'] = df.groupby(['sector', df['month'].dt.month])[target_col].transform(\n",
    "    lambda x: x.expanding().mean().shift(1)\n",
    ")\n",
    "\n",
    "# Time trend per sector (how much growth/decline over time)\n",
    "df['months_since_start'] = df.groupby('sector').cumcount()\n",
    "\n",
    "print(f\"sector_historical_mean: {df['sector_historical_mean'].notna().sum()}/{len(df)} non-null\")\n",
    "print(f\"sector_historical_std: {df['sector_historical_std'].notna().sum()}/{len(df)} non-null\")\n",
    "print(f\"month_historical_mean: {df['month_historical_mean'].notna().sum()}/{len(df)} non-null\")\n",
    "print(f\"sector_month_avg: {df['sector_month_avg'].notna().sum()}/{len(df)} non-null\")\n",
    "\n",
    "# Keep time features\n",
    "print(f\"\\nTime features: is_december, is_february, year, months_since_start\")\n",
    "\n",
    "# DON'T drop original features - keep them for future feature generation\n",
    "print(f\"\\nKeeping original features for future predictions\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_features = [col for col in df.columns if col not in ['month', 'date', 'sector', target_col]]\n",
    "print(f\"Final feature count: {len(all_features)}\")\n",
    "print(f\"Features: {all_features}\")\n",
    "print(f\"\\nRows with complete lag data: {df.dropna().shape[0]}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 3.7 Remove Data Leakage Columns\n",
    "# ------------------------------------------\n",
    "\n",
    "# These might directly calculate the target\n",
    "potential_leakage = [\n",
    "    'area_new_house_transactions',  # amount = area * price\n",
    "    'total_price_per_unit_new_house_transactions',  # directly related to amount\n",
    "    'price_new_house_transactions',   # directly related to amount\n",
    "    'price_pre_owned_house_transactions',   # directly related to amount\n",
    "    'quarter',  # might indirectly leak seasonal patterns\n",
    "    'area_new_house_available_for_sale',\n",
    "    'total_price_per_unit_pre_owned_house_transactions',  # directly related to amount\n",
    "    'month_num',  # might indirectly leak seasonal patterns\n",
    "    'num_new_house_transactions'    \n",
    "    'area_per_unit_new_house_transactions',  # ADD\n",
    "    'amount_new_house_transactions_nearby_sectors',  # ADD\n",
    "    'new_to_preowned_price_ratio'  # ADD\n",
    "]\n",
    "\n",
    "df.drop(columns=[col for col in potential_leakage if col in df.columns], inplace=True)\n",
    "\n",
    "print(f\"Remaining columns: {len(df.columns)}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3.8 Final Dataset Info\n",
    "# ------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Number of features: {len(df.columns) - 1}\")  # -1 for target\n",
    "\n",
    "# Save processed data\n",
    "output_path = Path(\"/Users/nikola/Python/KaggleCompetition/data\")\n",
    "df.to_csv(output_path / \"processed_data.csv\", index=False)\n",
    "print(f\"\\nProcessed data saved to: {output_path / 'processed_data.csv'}\")\n",
    "\n",
    "# Keep a copy for modeling\n",
    "df_processed = df.copy()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3.9 MAPE Competition Scoring Function\n",
    "# ------------------------------------------\n",
    "\n",
    "def custom_competition_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom competition scoring function.\n",
    "    \n",
    "    Stage 1: If more than 30% of samples have absolute percentage errors > 100%, return 0\n",
    "    Stage 2: Otherwise, calculate MAPE on samples with APE <= 1, scaled by fraction of valid samples\n",
    "    \n",
    "    Returns: Score between 0 and 1 (higher is better)\n",
    "    \"\"\"\n",
    "    # Calculate absolute percentage errors\n",
    "    ape = np.abs((y_pred - y_true) / y_true)\n",
    "    \n",
    "    # Stage 1: Check if more than 30% have errors > 100% (i.e., > 1.0)\n",
    "    extreme_errors = np.sum(ape > 1.0) / len(ape)\n",
    "    \n",
    "    if extreme_errors > 0.3:\n",
    "        return 0.0\n",
    "    \n",
    "    # Stage 2: Calculate MAPE on samples with APE <= 1\n",
    "    valid_mask = ape <= 1.0\n",
    "    \n",
    "    if np.sum(valid_mask) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # D: set of valid samples (APE <= 1)\n",
    "    valid_ape = ape[valid_mask]\n",
    "    \n",
    "    # Average MAPE of valid samples\n",
    "    mape_valid = np.mean(valid_ape)\n",
    "    \n",
    "    # Fraction of valid samples\n",
    "    fraction_valid = np.sum(valid_mask) / len(ape)\n",
    "    \n",
    "    # Scaled MAPE\n",
    "    scaled_mape = mape_valid / fraction_valid\n",
    "    \n",
    "    # Final score: 1 - scaled_mape\n",
    "    score = 1 - scaled_mape\n",
    "    \n",
    "    return max(0, score)  # Ensure non-negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c5d53",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cc281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. FEATURE CORRELATION ANALYSIS\n",
    "# ==========================================\n",
    "\n",
    "# Calculate correlation matrix for all features\n",
    "target_col = 'amount_new_house_transactions'\n",
    "non_features = ['month', 'date', 'sector', target_col]\n",
    "feature_cols = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = df[feature_cols + [target_col]].corr()\n",
    "\n",
    "# ==========================================\n",
    "# PLOT 1: Full Correlation Heatmap\n",
    "# ==========================================\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=False,  # Set to True if you want to see values (can be crowded)\n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            vmin=-1, \n",
    "            vmax=1,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# PLOT 2: High Correlation Pairs\n",
    "# ==========================================\n",
    "\n",
    "# Find highly correlated feature pairs (excluding target)\n",
    "correlation_threshold = 0.8\n",
    "feature_corr_matrix = df[feature_cols].corr()\n",
    "\n",
    "# Get upper triangle to avoid duplicates\n",
    "upper_triangle = np.triu(np.ones_like(feature_corr_matrix), k=1).astype(bool)\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(feature_corr_matrix.columns)):\n",
    "    for j in range(i+1, len(feature_corr_matrix.columns)):\n",
    "        corr_value = feature_corr_matrix.iloc[i, j]\n",
    "        if abs(corr_value) >= correlation_threshold:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': feature_corr_matrix.columns[i],\n",
    "                'Feature 2': feature_corr_matrix.columns[j],\n",
    "                'Correlation': corr_value\n",
    "            })\n",
    "\n",
    "# Display high correlation pairs\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', \n",
    "                                                               key=abs, \n",
    "                                                               ascending=False)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"HIGHLY CORRELATED FEATURE PAIRS (|correlation| >= {correlation_threshold})\")\n",
    "    print(\"=\"*70)\n",
    "    print(high_corr_df.to_string(index=False))\n",
    "    print(f\"\\nTotal pairs found: {len(high_corr_df)}\")\n",
    "    \n",
    "    # Visualize high correlation pairs\n",
    "    if len(high_corr_df) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(10, max(6, len(high_corr_df) * 0.4)))\n",
    "        colors = ['red' if abs(x) > 0.95 else 'orange' for x in high_corr_df['Correlation']]\n",
    "        ax.barh(range(len(high_corr_df)), high_corr_df['Correlation'], color=colors, alpha=0.7)\n",
    "        ax.set_yticks(range(len(high_corr_df)))\n",
    "        ax.set_yticklabels([f\"{row['Feature 1'][:20]}... vs {row['Feature 2'][:20]}...\" \n",
    "                            for _, row in high_corr_df.iterrows()], fontsize=8)\n",
    "        ax.set_xlabel('Correlation Coefficient')\n",
    "        ax.set_title(f'Highly Correlated Feature Pairs (|r| >= {correlation_threshold})')\n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"\\nNo feature pairs with |correlation| >= {correlation_threshold} found.\")\n",
    "\n",
    "# ==========================================\n",
    "# PLOT 3: Top Correlations with Target\n",
    "# ==========================================\n",
    "\n",
    "# Get correlations with target variable\n",
    "target_correlations = correlation_matrix[target_col].drop(target_col).sort_values(ascending=False)\n",
    "\n",
    "# Plot top positive and negative correlations\n",
    "n_top = 15\n",
    "top_positive = target_correlations.head(n_top)\n",
    "top_negative = target_correlations.tail(n_top)\n",
    "top_features = pd.concat([top_positive, top_negative]).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(8, len(top_features) * 0.3)))\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features]\n",
    "ax.barh(range(len(top_features)), top_features, color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features.index, fontsize=9)\n",
    "ax.set_xlabel('Correlation with Target')\n",
    "ax.set_title(f'Top {n_top} Positive and Negative Correlations with Target', fontsize=12)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION WITH TARGET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Highest positive correlation: {target_correlations.max():.4f} ({target_correlations.idxmax()})\")\n",
    "print(f\"Highest negative correlation: {target_correlations.min():.4f} ({target_correlations.idxmin()})\")\n",
    "print(f\"Mean absolute correlation: {target_correlations.abs().mean():.4f}\")\n",
    "print(f\"Features with |correlation| > 0.5: {(target_correlations.abs() > 0.5).sum()}\")\n",
    "print(f\"Features with |correlation| > 0.3: {(target_correlations.abs() > 0.3).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd822a2",
   "metadata": {},
   "source": [
    "# 4. PANEL DATA - CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44056d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4.1 CROSS-VALIDATION SETUP\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-VALIDATION SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort data by time (critical for time series)\n",
    "df = df.sort_values(['month', 'sector']).reset_index(drop=True)\n",
    "\n",
    "# Check data span\n",
    "print(f\"Data span: {df['month'].min()} to {df['month'].max()}\")\n",
    "print(f\"Total months: {df['month'].nunique()}\")\n",
    "print(f\"Total sectors: {df['sector'].nunique()}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Define features and target\n",
    "target_col = 'amount_new_house_transactions'\n",
    "non_features = ['month', 'date', 'sector', target_col]\n",
    "feature_cols = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "print(f\"\\nFeatures for model: {len(feature_cols)}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# Remove rows where target is NaN\n",
    "mask = ~y.isna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "print(f\"After removing NaN targets: {len(X)} rows\")\n",
    "\n",
    "# Simple TimeSeriesSplit\n",
    "# For monthly data: test_size = number of months to use as test\n",
    "# Let's use 6 months as test set for each fold\n",
    "tscv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    test_size=6 * df['sector'].nunique(),  # 6 months * number of sectors\n",
    "    gap=0  # No gap needed for monthly data\n",
    ")\n",
    "\n",
    "# Display the splits\n",
    "print(\"\\nCross-validation splits:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    train_months = df.iloc[train_idx]['month'].unique()\n",
    "    test_months = df.iloc[test_idx]['month'].unique()\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"  Train: {len(train_idx):,} rows ({len(train_months)} months)\")\n",
    "    print(f\"  Test:  {len(test_idx):,} rows ({len(test_months)} months)\")\n",
    "    print(f\"  Test period: {test_months.min()} to {test_months.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4.2 VISUALIZE CROSS-VALIDATION SPLITS\n",
    "# ==========================================\n",
    "\n",
    "# Since we have multiple sectors, let's aggregate by month for visualization\n",
    "monthly_target = df.groupby('month')[target_col].sum().reset_index()\n",
    "monthly_target = monthly_target.set_index('month').sort_index()\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(5, 1, figsize=(15, 12), sharex=True)\n",
    "fig.suptitle('Time Series Cross-Validation Splits', fontsize=14, y=1.02)\n",
    "\n",
    "# Plot each fold\n",
    "fold = 0\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    # Get train/test months\n",
    "    train_months = df.iloc[train_idx]['month'].unique()\n",
    "    test_months = df.iloc[test_idx]['month'].unique()\n",
    "    \n",
    "    # Filter monthly aggregated data\n",
    "    train_data = monthly_target[monthly_target.index.isin(train_months)]\n",
    "    test_data = monthly_target[monthly_target.index.isin(test_months)]\n",
    "    \n",
    "    # Plot\n",
    "    train_data.plot(ax=axes[fold], \n",
    "                    label='Training Set',\n",
    "                    color='#1f77b4',\n",
    "                    title=f'Fold {fold+1}: Train/Test Split')\n",
    "    test_data.plot(ax=axes[fold], \n",
    "                   label='Test Set',\n",
    "                   color='#ff7f0e')\n",
    "    \n",
    "    # Add vertical line at split point\n",
    "    axes[fold].axvline(test_data.index.min(), \n",
    "                       color='black', \n",
    "                       ls='--', \n",
    "                       alpha=0.7,\n",
    "                       label='Split Point')\n",
    "    \n",
    "    # Format\n",
    "    axes[fold].set_ylabel('Total Amount\\n(10k yuan)')\n",
    "    axes[fold].legend(loc='upper left')\n",
    "    axes[fold].grid(True, alpha=0.3)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "axes[-1].set_xlabel('Month')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SPLIT STATISTICS\")\n",
    "print(\"=\"*40)\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    train_y = y.iloc[train_idx]\n",
    "    test_y = y.iloc[test_idx]\n",
    "    print(f\"\\nFold {i+1}:\")\n",
    "    print(f\"  Train mean: {train_y.mean():,.0f}\")\n",
    "    print(f\"  Test mean:  {test_y.mean():,.0f}\")\n",
    "    print(f\"  Difference: {(test_y.mean() - train_y.mean())/train_y.mean()*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df55af",
   "metadata": {},
   "source": [
    "# 5. MODELING - XGBoost and Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5.1 XGBOOST MODELING WITH CROSS-VALIDATION\n",
    "# ==========================================\n",
    "\n",
    "# Define features and target\n",
    "target_col = 'amount_new_house_transactions'\n",
    "non_features = [\n",
    "    'month', \n",
    "    'date', \n",
    "    'sector', \n",
    "    'target_col',\n",
    "    'num_new_house_available_for_sale',\n",
    "    'amount_pre_owned_house_transactions',\n",
    "    'period_new_house_sell_through'\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target: {target_col}\")\n",
    "\n",
    "# Storage for results\n",
    "fold = 0\n",
    "preds = []\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(df):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "    \n",
    "    # Split data\n",
    "    train = df.iloc[train_idx]\n",
    "    test = df.iloc[test_idx]\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X_train = train[feature_cols]\n",
    "    y_train = train[target_col]\n",
    "    X_test = test[feature_cols]\n",
    "    y_test = test[target_col]\n",
    "    \n",
    "    # Handle NaN in target\n",
    "    train_mask = ~y_train.isna()\n",
    "    test_mask = ~y_test.isna()\n",
    "    \n",
    "    X_train = X_train[train_mask]\n",
    "    y_train = y_train[train_mask]\n",
    "    X_test = X_test[test_mask]\n",
    "    y_test = y_test[test_mask]\n",
    "    \n",
    "    print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "    \n",
    "    # XGBoost model with early_stopping_rounds in constructor\n",
    "    reg = xgb.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='mape',\n",
    "        early_stopping_rounds=50  # Move it here\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    reg.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = reg.predict(X_test)\n",
    "    preds.append(y_pred)\n",
    "    \n",
    "    # Calculate competition score using custom function\n",
    "    comp_score = custom_competition_score(y_test.values, y_pred)\n",
    "    scores.append(comp_score)\n",
    "    models.append(reg)\n",
    "    \n",
    "    # Also calculate RMSE for reference\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Diagnostic info\n",
    "    ape = np.abs((y_pred - y_test.values) / y_test.values)\n",
    "    extreme_pct = 100 * np.sum(ape > 1.0) / len(ape)\n",
    "    valid_pct = 100 * np.sum(ape <= 1.0) / len(ape)\n",
    "    \n",
    "    print(f\"Competition Score: {comp_score:.4f}\")\n",
    "    print(f\"RMSE: {rmse:,.0f}\")\n",
    "    print(f\"Samples with APE > 100%: {extreme_pct:.1f}%\")\n",
    "    print(f\"Samples with APE ≤ 100%: {valid_pct:.1f}%\")\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"Average Competition Score: {np.mean(scores):.4f}\")\n",
    "print(f\"Std Competition Score: {np.std(scores):.4f}\")\n",
    "print(f\"Best Competition Score: {np.max(scores):.4f}\")\n",
    "print(f\"Worst Competition Score: {np.min(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eacba9",
   "metadata": {},
   "source": [
    "# 6. PREDICTING THE FUTURE\n",
    "Retraining on all data\n",
    "\n",
    "To Predict the future we need an emtpy dataframe for future date ranges.\n",
    "\n",
    "Run those dates through our feature creation code + lag creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. PREDICTIONS FOR SUBMISSION\n",
    "# ==========================================\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6.1 Load and Decode Submission Template\n",
    "# ------------------------------------------\n",
    "\n",
    "submission_template = pd.read_csv(\"/Users/nikola/Python/KaggleCompetition/data/sample_submission.csv\")\n",
    "print(f\"Submission shape: {submission_template.shape}\")\n",
    "\n",
    "# Parse the ID format\n",
    "submission_template['year'] = submission_template['id'].str.split(' ').str[0].astype(int)\n",
    "submission_template['month_abbr'] = submission_template['id'].str.split('_').str[0].str.split(' ').str[1]\n",
    "submission_template['sector'] = submission_template['id'].str.split('_').str[1]\n",
    "submission_template['month'] = pd.to_datetime(\n",
    "    submission_template['year'].astype(str) + ' ' + submission_template['month_abbr'],\n",
    "    format='%Y %b'\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6.2 Prepare Original Features for Future Months\n",
    "# ------------------------------------------\n",
    "\n",
    "# Create a dataframe with all sector-month combinations\n",
    "future_base = submission_template[['month', 'sector']].copy()\n",
    "\n",
    "# Get the last known values for each sector for the original features\n",
    "last_values = df.groupby('sector')[['num_new_house_available_for_sale',\n",
    "                                     'amount_pre_owned_house_transactions', \n",
    "                                     'period_new_house_sell_through']].last()\n",
    "\n",
    "# Merge these values with future months (assume they stay constant)\n",
    "future_base = future_base.merge(last_values, left_on='sector', right_index=True, how='left')\n",
    "\n",
    "# For the new sector 95, fill with overall median\n",
    "for col in ['num_new_house_available_for_sale', 'amount_pre_owned_house_transactions', 'period_new_house_sell_through']:\n",
    "    future_base.loc[:, col] = future_base[col].fillna(df[col].median())\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6.3 Create Combined Dataset\n",
    "# ------------------------------------------\n",
    "\n",
    "# Combine historical data with future data\n",
    "combined = pd.concat([\n",
    "    df,\n",
    "    future_base\n",
    "], ignore_index=True)\n",
    "\n",
    "combined = combined.sort_values(['sector', 'month']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined shape: {combined.shape}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6.4 Regenerate ALL Features on Combined Dataset\n",
    "# ------------------------------------------\n",
    "\n",
    "# Time features\n",
    "combined['is_december'] = (combined['month'].dt.month == 12).astype(int)\n",
    "combined['is_february'] = (combined['month'].dt.month == 2).astype(int)\n",
    "combined['year'] = combined['month'].dt.year\n",
    "\n",
    "# Lag features (will use historical values for future months)\n",
    "for feature in ['num_new_house_available_for_sale', 'amount_pre_owned_house_transactions', 'period_new_house_sell_through']:\n",
    "    combined[f'{feature}_lag1'] = combined.groupby('sector')[feature].shift(1)\n",
    "\n",
    "# Time series features\n",
    "combined['sector_historical_mean'] = combined.groupby('sector')[target_col].transform(\n",
    "    lambda x: x.expanding().mean().shift(1)\n",
    ")\n",
    "combined['sector_historical_std'] = combined.groupby('sector')[target_col].transform(\n",
    "    lambda x: x.expanding().std().shift(1)\n",
    ")\n",
    "combined['month_historical_mean'] = combined.groupby(combined['month'].dt.month)[target_col].transform(\n",
    "    lambda x: x.expanding().mean().shift(1)\n",
    ")\n",
    "combined['sector_month_avg'] = combined.groupby(['sector', combined['month'].dt.month])[target_col].transform(\n",
    "    lambda x: x.expanding().mean().shift(1)\n",
    ")\n",
    "combined['months_since_start'] = combined.groupby('sector').cumcount()\n",
    "\n",
    "# Define features (same as training)\n",
    "target_col = 'amount_new_house_transactions'\n",
    "non_features = [\n",
    "    'month', 'date', 'sector', target_col,\n",
    "    'num_new_house_available_for_sale',\n",
    "    'amount_pre_owned_house_transactions',\n",
    "    'period_new_house_sell_through'\n",
    "]\n",
    "feature_cols = [col for col in combined.columns if col not in non_features]\n",
    "\n",
    "print(f\"\\nInitial feature count: {len(feature_cols)}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6.5 Get Submission Data and Filter Features\n",
    "# ------------------------------------------\n",
    "\n",
    "# Get submission rows FIRST\n",
    "submission_mask = combined['month'].isin(submission_template['month'])\n",
    "X_submission_temp = combined[submission_mask][feature_cols]\n",
    "\n",
    "# NOW filter features based on missing data\n",
    "features_with_data = []\n",
    "for col in feature_cols:\n",
    "    missing_pct = X_submission_temp[col].isna().sum() / len(X_submission_temp)\n",
    "    if missing_pct < 0.9:  # Keep features with <90% missing\n",
    "        features_with_data.append(col)\n",
    "    else:\n",
    "        print(f\"Removing {col}: {missing_pct*100:.1f}% missing\")\n",
    "\n",
    "feature_cols = features_with_data\n",
    "print(f\"\\nUsing {len(feature_cols)} features after filtering\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6.6 Train Final Model\n",
    "# ------------------------------------------\n",
    "\n",
    "train_mask = combined[target_col].notna()\n",
    "X_train = combined[train_mask][feature_cols].dropna()\n",
    "y_train = combined[train_mask][target_col].loc[X_train.index]\n",
    "\n",
    "print(f\"\\nTraining on {len(X_train)} samples\")\n",
    "\n",
    "final_model = xgb.XGBRegressor(\n",
    "    n_estimators=974,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03397760168814681,\n",
    "    subsample=0.8701413713402404,\n",
    "    colsample_bytree=0.8128659290802385,\n",
    "    min_child_weight=9,\n",
    "    gamma=1.13656860535782,\n",
    "    random_state=42,\n",
    "    eval_metric='mape'\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6.7 Make Predictions\n",
    "# ------------------------------------------\n",
    "\n",
    "# Get submission rows with filtered features\n",
    "X_submission = combined[submission_mask][feature_cols].copy()\n",
    "\n",
    "print(f\"\\nSubmission features - Missing values:\")\n",
    "print(X_submission.isna().sum())\n",
    "\n",
    "# Fill any remaining NaN with training medians\n",
    "for col in feature_cols:\n",
    "    if X_submission[col].isna().any():\n",
    "        X_submission.loc[:, col] = X_submission[col].fillna(X_train[col].median())\n",
    "\n",
    "predictions = final_model.predict(X_submission)\n",
    "\n",
    "# Clip negative predictions to 0\n",
    "predictions = np.maximum(predictions, 0)\n",
    "\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"  Min: {predictions.min():,.0f}\")\n",
    "print(f\"  Max: {predictions.max():,.0f}\")\n",
    "print(f\"  Mean: {predictions.mean():,.0f}\")\n",
    "print(f\"  Non-zero: {(predictions > 0).sum()}/{len(predictions)}\")\n",
    "\n",
    "# Create submission\n",
    "submission_template['amount'] = predictions\n",
    "submission_template[['id', 'amount']].to_csv(\"/Users/nikola/Python/KaggleCompetition/output/XGBOOST/submission.csv\", index=False)\n",
    "print(\"\\n✓ Submission file saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kagglecompetition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
